{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(0,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tuple([a[x] if x>0 else 99 for x in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 3, 4)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    Train = pd.read_csv('diabetes.csv')\n",
    "    Train.head()\n",
    "    print(pd.isnull(Train).any().any())\n",
    "    if(pd.isnull(Train).any().any()):\n",
    "        print(\" Error :missing value in dataset\")\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    X =Train[Train.columns[:-1]]\n",
    "    y=Train[Train.columns[-1]]\n",
    "\n",
    "    x = X.values #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    X = x_scaled\n",
    "    y=y.values\n",
    "    x.shape\n",
    "    return X,y\n",
    "X,y=train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# direct data without preprocessing \n",
    "y=y\n",
    "X=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.20, random_state=1994)\n",
    "X_train, X_test, y_train, y_test=np.array(X_train).T, \\\n",
    "np.array(X_test).T, np.array(y_train).reshape(1,-1), np.array(y_test).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 614)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \n",
    "    return 1/(1+np.exp(-Z)), Z\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z), Z\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[cache <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_n_layes(layer_dims):\n",
    "    parameters = {}\n",
    "    N = len(layer_dims)            # number of layers in the network\n",
    "    \n",
    "    # * np.sqrt(1/layer_dims[n]) for gradient vanishing and exploding \n",
    "    \n",
    "    # intialized parameters layer by layer\n",
    "    for n in range(1, N):\n",
    "        parameters['W' + str(n)] = np.random.randn(layer_dims[n], layer_dims[n-1]) * np.sqrt(1/layer_dims[n])\n",
    "        parameters['b' + str(n)] = np.zeros((layer_dims[n], 1))*np.sqrt(1/layer_dims[n])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov1D(X,W,activation='relu'):\n",
    "    \"\"\"\n",
    "    X -- output of the previous layer, numpy array of shape (n_H_prev, n_W_prev) \n",
    "    W -- Weights, numpy array of size (f) assuming number of filters = 1\n",
    "    new_matrix is output convolution\n",
    "    cache : cache needed for backpropgation\n",
    "    \"\"\"\n",
    "    W_row= W.shape[0]\n",
    "    X_row, x_number=X.shape\n",
    "    # new row and col\n",
    "    new_row =X_row -W_row+1\n",
    "    new_matrix = np.zeros((new_row,x_number))\n",
    "\n",
    "    for start_row in  range(new_row):\n",
    "        new_matrix[start_row,:]=np.sum(X[start_row:start_row+W_row,:]*W.reshape(-1,1),axis=0)\n",
    "    # if odd output then add zero (padding) to make even  \n",
    "\n",
    "    if new_matrix.shape[0]%2==1:\n",
    "        new_matrix=np.concatenate((new_matrix,np.zeros((1,new_matrix.shape[1]))), axis=0)\n",
    "    \n",
    "    \n",
    "    linera_cache=(X,W)\n",
    "    if activation=='relu':\n",
    "        A, activation_cache = relu(new_matrix)\n",
    "\n",
    "        \n",
    "    cache =(linera_cache,activation_cache)\n",
    "    \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1, ...,  1,  1,  1],\n",
       "       [ 2,  2,  2, ...,  2,  2,  2],\n",
       "       [ 9,  9,  9, ...,  9,  9,  9],\n",
       "       [13,  6, 13, ..., 13, 13,  6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pool_1D(X,size=2,method='max'):\n",
    "    X_row,x_number =X.shape\n",
    "    # new row and according to filter size\n",
    "    new_row =X_row//size \n",
    "    \n",
    "    new_matrix = np.zeros((new_row,x_number))\n",
    "    index_matrix=np.zeros((new_row,x_number))\n",
    "    \n",
    "    for start_row in  range(new_row):\n",
    "            if method=='max':\n",
    "                value=np.max(X[start_row*size:start_row*size+size,:],axis=0)\n",
    "                i=(1+np.argmax(X[start_row*size:start_row*size+size,:],axis=0))*(start_row*size+1)\n",
    "            else:\n",
    "                value=np.mean(X[start_row*size:start_row*size+size,:],axis=0)\n",
    "                i=(1+np.argmax(X[start_row*size:start_row*size+size,:],axis=0))*(start_row*size+1)\n",
    "                \n",
    "            new_matrix[start_row]=value\n",
    "            index_matrix[start_row]=i-1    \n",
    "            \n",
    "    cache=(X,method,size,index_matrix.astype(int))\n",
    "    \n",
    "    return new_matrix ,cache\n",
    "t=pool_1D(X_train)\n",
    "t[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W,A)+ b \n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dH, cache,activation):\n",
    "    '''\n",
    "    The backward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dH -- gradient of the cost with respect to output of the conv layer (H) or first fully donnected layer, numpy array of shape (n_H, n_W) assuming channels = 1\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W), assuming single filter\n",
    "    '''\n",
    "     \n",
    "        \n",
    "    (linera_cache,activation_cache)= cache\n",
    "    \n",
    "    \n",
    "    (X,W)=linera_cache\n",
    "    \n",
    "    # Retrieving information from the \"cache\"\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    n_H_prev,n_number = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    f = W.shape[0]\n",
    "    \n",
    "    # Retrieving dimensions from dH's shape\n",
    "    n_H = dH.shape[0]\n",
    "    \n",
    "    # Initializing dX, dW with the correct shapes\n",
    "    dX = np.zeros(X.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    m = dH.shape[1]\n",
    "    \n",
    "    # Looping over horizontal(h)  axis of the output\n",
    "    \n",
    "    # back prop to activation function\n",
    "    if activation =='relu':\n",
    "        dZ = relu_backward(dH, activation_cache)\n",
    "    \n",
    "\n",
    "    for h in range(f-1):\n",
    "        dW[h,:]= np.sum(X[h:h+n_H,:]*dZ)/m\n",
    "    # n_H_pre heigh of X (data)\n",
    "    # n_H height of dH (pool results)\n",
    "    # f  height of W (filter)\n",
    "    m=0\n",
    "    size_of_w=0\n",
    "    for i in range(n_H_prev):\n",
    "        if f>i:\n",
    "            \n",
    "            dX[i,:]=np.sum(dH[:i+1,:]*np.flip(W[:i+1],axis=0),axis=0)\n",
    "        elif f<=i and i<n_H:\n",
    "            m=m+1\n",
    "            dX[i,:]=np.sum(dH[m:m+f,:]*np.flip(W,axis=0),axis=0)\n",
    "        else:\n",
    "            m=m+1\n",
    "            size_of_w+=1\n",
    "            dX[i,:]=np.sum(dH[m:,:]*np.flip(W[size_of_w:],axis=0),axis=0)\n",
    "    return dX, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_w=0\n",
    "size_of_w+=11\n",
    "size_of_w+=11\n",
    "size_of_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dH, cache):\n",
    "        \n",
    "    # Retrieving information from the \"cache\"\n",
    "    (X,method,size,index_matrix)= cache\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    n_H_prev,n_number = X.shape\n",
    "\n",
    "    \n",
    "    # Retrieving dimensions from dH's shape\n",
    "    n_H = dH.shape[0]\n",
    "    m = dH.shape[1]\n",
    "    \n",
    "    \n",
    "    # Initializing dX\n",
    "    dX = np.zeros((dH.shape[0]*size,n_number))\n",
    "\n",
    "    #There is no gradient with respect to non maximum values so only max is locally linear with slope 1\n",
    "    \n",
    "    for i in range(n_H):\n",
    "        dX[index_matrix[0],np.arange(0,index_matrix.shape[1],1)]=dH[i]\n",
    "    \n",
    "    \n",
    "    return dX, method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(steps,learning_rate):\n",
    "    initial_lrate = learning_rate\n",
    "    drop = 0.2\n",
    "    epochs_drop = 2\n",
    "    learning_rate = initial_lrate * np.power(drop,np.floor((1+steps)/epochs_drop))\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dims = (45,300,20, 1)\n",
    "'C'+str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_model(X, Y, layers_dims, learning_rate = .001, num_iterations = 500):\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost \n",
    "    m = X.shape[1]                           # number of examples\n",
    "    \n",
    "    # Initialize parameters dictionary\n",
    "    parameters = initialize_parameters_n_layes(layers_dims)\n",
    "    \n",
    "    #parameters['W0']=((np.random.rand(5)-1/2)*2).reshape(5,1)\n",
    "\n",
    "    \n",
    "    \n",
    "    intial_learning_rate=learning_rate\n",
    "    total_filters=4\n",
    "    parameters['total_filters']=total_filters\n",
    "    for i in range(total_filters):\n",
    "        np.random.seed(10)\n",
    "        parameters['C'+str(i)]=np.random.uniform(0,1,5).reshape(5,1)\n",
    "        parameters['P'+str(i)]=2\n",
    "    \n",
    "        \n",
    "    cost_pre=0\n",
    "    #return parameters\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        \n",
    "        # 2 filters\n",
    "        #sore all cache\n",
    "        cache_conv = []\n",
    "        cache_pool = []\n",
    "        pool_size= []\n",
    "\n",
    "      \n",
    "        value=X\n",
    "        fully_connect=0\n",
    "        for n in range(total_filters):\n",
    "            C, cachec  = cov1D(X,parameters['C'+str(n)],activation='relu')    \n",
    "            P, cachep  = pool_1D(C,parameters['P'+str(n)],method='max')\n",
    "            value=C     # update value \n",
    "            cache_conv.append(cachec),cache_pool.append(cachep)\n",
    "            pool_size.append(P.shape[0])\n",
    "            try:\n",
    "                fully_connect = np.concatenate((fully_connect,P),axis=0)\n",
    "            except:\n",
    "                fully_connect = P\n",
    "        \n",
    "        A1, cache1 = linear_activation_forward(fully_connect, parameters[\"W1\"], parameters[\"b1\"], \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, parameters[\"W2\"], parameters[\"b2\"], \"relu\")\n",
    "        A3, cache3 = linear_activation_forward(A2, parameters[\"W3\"], parameters[\"b3\"], \"sigmoid\")\n",
    "        \n",
    "       \n",
    "        # Compute cost \n",
    "        cost = compute_cost(A3, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3))\n",
    "        \n",
    "        # Backward propagation. \n",
    "        dA2, dW3, db3 = linear_activation_backward(dA3, cache3, \"sigmoid\")\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"relu\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "\n",
    "        # pooling \n",
    "        value=dA0\n",
    "        \n",
    "        end=0\n",
    "        for n in range(total_filters):\n",
    "            top = end\n",
    "            end=pool_size[n]+top\n",
    "            dAp, method = pool_backward(value[top:end,:], cache_pool[n])\n",
    "            dAc, dC = conv_backward(dAp, cache_conv[n],'relu')\n",
    "            grads['dC'+str(n)]=dC\n",
    "     \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'],grads['db1'],grads['dW2'],grads['db2']  = dW1, db1, dW2, db2\n",
    "        grads['dW3'],grads['db3']=dW3,db3\n",
    "        # Update parameters W1, b1, W2, b2\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # decay learning rate\n",
    "        \n",
    "       # if abs(cost-cost_pre)<0.00001:\n",
    "        #    learning_rate=learning_rate/1.5\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        cost_pre=cost\n",
    "        # Print the cost \n",
    "        step=2000 # store after step iterations\n",
    "        if i % step == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % step == 0:\n",
    "            costs.append(cost)\n",
    "            print('train {} test {}'.format(predict(X_train, y_train,parameters,layers_dims),predict(X_test, y_test,parameters,layers_dims)))\n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations ({})'.format(step))\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    # return updated paramenters\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_parameters_n_layer_conv(conv_layer_dims,parameters):\n",
    "    pre=1\n",
    "    for l in range(len(conv_layer_dims)):\n",
    "        \n",
    "        # filters no used in eqach layer and size\n",
    "        \n",
    "        (parameters['filters'+str(l)],parameters[\"f_size\"+str(l)])=conv_layer_dims[l]\n",
    "        pre=parameters[\"filters\"+str(l)]*pre\n",
    "        \n",
    "        # total filter in each layer 2X2X5=20\n",
    "        parameters['no_filters'+str(l)]=pre\n",
    "        \n",
    "        for n in range(parameters['filters'+str(l)]):\n",
    "            # layer the number of filter\n",
    "            np.random.seed(8)\n",
    "            parameters['C'+str(l)+str(n)]=np.random.uniform(0,1,\n",
    "                                                     parameters[\"f_size\"+\\\n",
    "                                                                str(l)]).reshape(parameters[\"f_size\"+str(l)],\n",
    "                                                                                          1)*np.sqrt(1/parameters[\"f_size\"+str(l)])\n",
    "            parameters['P'+str(l)+str(n)]=2\n",
    "            print(parameters['C'+str(l)+str(n)])\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECt check with tree layer \n",
    "def cov_parrale(parameters,conv_layer_dims,X):\n",
    "\n",
    "    tree = {}\n",
    "    tree['X']=X\n",
    "    total_filters=1 # for start\n",
    "\n",
    "    for l in range(len(conv_layer_dims)):\n",
    "        try:\n",
    "            pre_filters=parameters['filters'+str(l-1)]\n",
    "        except:\n",
    "            pre_filters=1\n",
    "        # apply no of filters\n",
    "        total_filters=parameters['filters'+str(l)] \n",
    "        # store all cache of conv and pool values \n",
    "        tree['C'+str(l)]=[]\n",
    "        tree['P'+str(l)]=[]\n",
    "        tree['F'+str(l)]=[]\n",
    "        for node in range(pre_filters):\n",
    "            # for all sub filter after layer \n",
    "            try:\n",
    "                # extract  node values \n",
    "                # pre layer all  values after Applyied conv and pooling \n",
    "                value=tree['F'+str(l-1)][node]\n",
    "\n",
    "\n",
    "            except:\n",
    "                value=tree['X'] # if first layer\n",
    "\n",
    "            # filter one by one on one pre pool result \n",
    "\n",
    "            for n in range(total_filters):\n",
    "                # same for all \n",
    "                loc=str(l)+str(n)\n",
    "                C, cachec  = cov1D(value,parameters['C'+loc],activation='relu')    \n",
    "                P, cachep  = pool_1D(C,parameters['P'+loc],method='max')\n",
    "                # unique name by layer and new node \n",
    "                tree['C'+str(l)].append(cachec)\n",
    "                tree['P'+str(l)].append(cachep)\n",
    "                tree['F'+str(l)].append(P)\n",
    "                # one child of each node \n",
    "\n",
    "            # cache save from layers \n",
    "    return tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back prop conv layers \n",
    "\n",
    "def back_prop_cov_layer(parameters,conv_layer_dims,tree,value,grads):\n",
    "    \n",
    "    back_value=value\n",
    "    for layer in reversed(range(len(conv_layer_dims))):\n",
    "        \n",
    "        filters=parameters['filters'+str(layer)]\n",
    "        total_no_filters=parameters['no_filters'+str(layer)]\n",
    "        \n",
    "        sub_filter =total_no_filters//filters\n",
    "\n",
    "        cache_conv=tree['C'+str(layer)]\n",
    "        cache_pool=tree['P'+str(layer)]\n",
    "        cache_pool_value=tree['F'+str(layer)]\n",
    "        #  total_no_filters in layer l\n",
    "        n=0\n",
    "        value=back_value\n",
    "        back_value=0\n",
    "        end=0\n",
    "        for node in range(sub_filter):\n",
    "            for f in range(filters):\n",
    "                # start location \n",
    "                top=0+end\n",
    "                # end location \n",
    "                end=cache_pool_value[n].shape[0]+top\n",
    "                dAp, method = pool_backward(value[top:end,:], cache_pool[n])\n",
    "                dAc, dC = conv_backward(dAp, cache_conv[n],'relu')\n",
    "                n=n+1\n",
    "                # Set grads\n",
    "                try:\n",
    "                    grads['dC'+str(layer)+str(f)]+=dC\n",
    "                except:\n",
    "                    grads['dC'+str(layer)+str(f)]=dC\n",
    "                # store value for backprop\n",
    "                try:\n",
    "                    back_value=np.concatenate((back_value,dAc),axis=0)\n",
    "                except:\n",
    "                    back_value=dAc\n",
    "                \n",
    "                \n",
    "                \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_layer_model(X, Y, layers_dims,conv_layer_dims, learning_rate = 0.1, num_iterations = 500):\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost \n",
    "    m = X.shape[1]                           # number of examples\n",
    "    train_test=[]\n",
    "    # Initialize parameters dictionary\n",
    "    parameters = initialize_parameters_n_layes(layers_dims)\n",
    "    np.random.seed(8)\n",
    "    # no of filter + size of filter\n",
    "    \n",
    "    total_filters=3\n",
    "    intialize_parameters_n_layer_conv(conv_layer_dims,parameters)\n",
    "\n",
    "    \n",
    "    learning_rate_i=learning_rate\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "\n",
    "        #sore all cache\n",
    "        # for all layers \n",
    "        tree = cov_parrale(parameters,conv_layer_dims,X)\n",
    "    \n",
    "                \n",
    "        layer=len(conv_layer_dims)-1\n",
    "        value=np.concatenate([tree['F'+str(layer)][x] for x in  range(parameters['no_filters'+str(layer)])],axis=0)\n",
    "        cache_ANN=[]\n",
    "        for n in range(1,len(layers_dims)):\n",
    "            \n",
    "            if n<len(layers_dims)-1:\n",
    "                A, cache = linear_activation_forward(value, parameters[\"W\"+str(n)], parameters[\"b\"+str(n)], \"relu\")\n",
    "            else:\n",
    "                A, cache = linear_activation_forward(value, parameters[\"W\"+str(n)], parameters[\"b\"+str(n)], \"sigmoid\")\n",
    "            value=A\n",
    "            cache_ANN.append(cache)\n",
    "       \n",
    "        # Compute cost \n",
    "        cost = compute_cost(A, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "        \n",
    "        # Backward propagation. \n",
    "        \n",
    "        # fully connected layer \n",
    "        for n in reversed(range(1,len(layers_dims))):\n",
    "            if n==len(layers_dims)-1:\n",
    "                dA , dW , db =linear_activation_backward(dA,cache_ANN[n-1],\"sigmoid\")\n",
    "            else:\n",
    "                dA , dW , db =linear_activation_backward(dA,cache_ANN[n-1],\"relu\")\n",
    "            # Set grads\n",
    "            grads[\"dW\"+str(n)]=dW\n",
    "            grads[\"db\"+str(n)]=db\n",
    "       \n",
    "        # pooling and conv back prop \n",
    "        grads=back_prop_cov_layer(parameters,conv_layer_dims,tree,dA,grads)\n",
    "        \n",
    "        # Update parameters W1, b1, W2, b2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate,conv_layer_dims)\n",
    "        \n",
    "\n",
    "        step=50 # store after step iterations  \n",
    "        if i % step == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            costs.append(cost)\n",
    "            train=predict(X_train, y_train,parameters,layers_dims,conv_layer_dims)\n",
    "            test=predict(X_test, y_test,parameters,layers_dims,conv_layer_dims)\n",
    "                                                             \n",
    "    \n",
    "            print('train {} test {} learning_rate {}'.format(train,test,learning_rate))\n",
    "            #learning_rate =step_decay(i/100,learning_rate_i)\n",
    "            train_test.append((train,test))\n",
    "            \n",
    "            #learning_rate =step_decay(i/100,learning_rate_i)\n",
    "   \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations ({})'.format(step))\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    # return updated paramenters\n",
    "    return parameters,tree,train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    \n",
    "    #  prevent  NaN values in your cost function\n",
    "    AL[np.where(AL==0)]=0.00001\n",
    "    AL[np.where(AL==1)]=0.99999\n",
    "    \n",
    "    \n",
    "    cost = -1/m * np.sum(np.multiply(np.log(AL), Y) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW =  1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ,axis = 1,keepdims = True )\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- Gradient with respect to ;\n",
    "    cache -- tuple of values (A_prev, W, b) from forward propagation\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with prev layer\n",
    "    dW -- db  Gradient\n",
    "    \"\"\"\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    #Gradient with respect to prev and Gradient\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(parameters, grads, learning_rate,conv_layer_dims):\n",
    "    \n",
    "    N = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update prameters W1,W2,b1,b2\n",
    "    for n in range(len(layers_dims)-1):\n",
    "        parameters[\"W\" + str(n+1)] = parameters[\"W\" + str(n+1)] - learning_rate * grads[\"dW\" + str(n+1)]\n",
    "        parameters[\"b\" + str(n+1)] = parameters[\"b\" + str(n+1)] - learning_rate * grads[\"db\" + str(n+1)]\n",
    "    # covolution neural network \n",
    "    for layer in range(len(conv_layer_dims)):\n",
    "        for  n in range(parameters['filters'+str(layer)]):\n",
    "            parameters[\"C\"+str(layer)+str(n)] = parameters[\"C\"+str(layer)+str(n)] - learning_rate * grads[\"dC\"+str(layer)+str(n)]\n",
    "    #print(grads[\"dC0\"] )\n",
    "        \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters,layers_dims,conv_layer_dims):\n",
    "    m = X.shape[1] # number of inputes\n",
    "    predictions = np.zeros((1,m)) # class\n",
    "    \n",
    "    # Forward propagation\n",
    "    tree = cov_parrale(parameters,conv_layer_dims,X)\n",
    "    \n",
    "    layer=len(conv_layer_dims)-1\n",
    "    \n",
    "    value=np.concatenate([tree['F'+str(layer)][x] for x in  range(parameters['no_filters'+str(layer)])],axis=0)\n",
    "    for n in range(1,len(layers_dims)):\n",
    "        if n<len(layers_dims)-1:\n",
    "            A, cache = linear_activation_forward(value, parameters[\"W\"+str(n)], parameters[\"b\"+str(n)], \"relu\")\n",
    "        else:\n",
    "            A, cache = linear_activation_forward(value, parameters[\"W\"+str(n)], parameters[\"b\"+str(n)], \"sigmoid\")\n",
    "        value=A\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[:,i]>0.5:\n",
    "            predictions[:,i]=1\n",
    "        else:\n",
    "            predictions[:,i]=0\n",
    "    \n",
    "        \n",
    "    return  str(np.sum((predictions == y)/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5042747 ]\n",
      " [0.55918721]\n",
      " [0.5018297 ]]\n",
      "[[0.5042747 ]\n",
      " [0.55918721]\n",
      " [0.5018297 ]]\n",
      "[[0.5042747 ]\n",
      " [0.55918721]\n",
      " [0.5018297 ]]\n",
      "[[0.8734294]]\n",
      "[[0.8734294]]\n",
      "Cost after iteration 0: 1.0638544122545788\n",
      "train 0.3403908794788274 test 0.38311688311688313 learning_rate 0.1\n",
      "Cost after iteration 50: 0.580808408908545\n",
      "train 0.6905537459283388 test 0.668831168831169 learning_rate 0.1\n",
      "Cost after iteration 100: 0.5511121793163658\n",
      "train 0.719869706840391 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 150: 0.5405474530768251\n",
      "train 0.731270358306189 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 200: 0.5358719774290368\n",
      "train 0.721498371335505 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 250: 0.5332345370316383\n",
      "train 0.716612377850163 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 300: 0.5307166864301157\n",
      "train 0.719869706840391 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 350: 0.5277912068924296\n",
      "train 0.714983713355049 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 400: 0.5264683944630657\n",
      "train 0.718241042345277 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 450: 0.5254444673020492\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 500: 0.5244741914240114\n",
      "train 0.721498371335505 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 550: 0.5235967335435942\n",
      "train 0.721498371335505 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 600: 0.5228628447211454\n",
      "train 0.721498371335505 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 650: 0.5221291020276628\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 700: 0.5213332451884356\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 750: 0.5206559089143867\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 800: 0.5199953134001241\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 850: 0.5193889399462874\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 900: 0.5188317353995197\n",
      "train 0.719869706840391 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 950: 0.5182899048276921\n",
      "train 0.719869706840391 test 0.7077922077922079 learning_rate 0.1\n",
      "Cost after iteration 1000: 0.5177228654421046\n",
      "train 0.719869706840391 test 0.7077922077922079 learning_rate 0.1\n",
      "Cost after iteration 1050: 0.5170920861088849\n",
      "train 0.724755700325733 test 0.7012987012987014 learning_rate 0.1\n",
      "Cost after iteration 1100: 0.5165967194644255\n",
      "train 0.726384364820847 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 1150: 0.5161411993823213\n",
      "train 0.726384364820847 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 1200: 0.5157009069187394\n",
      "train 0.724755700325733 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 1250: 0.5152325399982547\n",
      "train 0.726384364820847 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1300: 0.5148052158965527\n",
      "train 0.729641693811075 test 0.694805194805195 learning_rate 0.1\n",
      "Cost after iteration 1350: 0.5144140087497591\n",
      "train 0.729641693811075 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1400: 0.5140327079303622\n",
      "train 0.728013029315961 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1450: 0.5136689030089664\n",
      "train 0.726384364820847 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1500: 0.5133077056662477\n",
      "train 0.724755700325733 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1550: 0.5128246867483404\n",
      "train 0.724755700325733 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1600: 0.5123899014107756\n",
      "train 0.724755700325733 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1650: 0.5119956954806673\n",
      "train 0.724755700325733 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1700: 0.5116369027422524\n",
      "train 0.723127035830619 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1750: 0.5113011884353772\n",
      "train 0.721498371335505 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1800: 0.5109546385518307\n",
      "train 0.721498371335505 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1850: 0.5106321913378169\n",
      "train 0.721498371335505 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1900: 0.5103198596032615\n",
      "train 0.721498371335505 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 1950: 0.5100254510909986\n",
      "train 0.719869706840391 test 0.6883116883116884 learning_rate 0.1\n",
      "Cost after iteration 2000: 0.5097329204142976\n",
      "train 0.719869706840391 test 0.6883116883116884 learning_rate 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dims = (12,100, 1)\n",
    "conv_layer_dims=[(3,3),(2,1)]\n",
    "parameters,tree ,train_test= N_layer_model(X_train,y_train,\n",
    "                                layers_dims = layers_dims,\n",
    "                                conv_layer_dims=conv_layer_dims,\n",
    "                                num_iterations = 2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train,test =zip(*train_test)\n",
    "plt.plot(np.squeeze(np.around(np.array(train).astype(float),decimals=4)))\n",
    "plt.plot(np.around(np.array(test).astype(float),decimals=4))\n",
    "plt.legend(['train','test'])\n",
    "plt.ylabel('train,test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
